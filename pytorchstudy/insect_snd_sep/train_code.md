# Explain training code and config

The train.py and yaml config work together to complete the whole logic.

## Main logic of train.py

train.py takes a config yaml as the main parameter

in __main__, parse yaml file, init distribute computing if applicable.

call 'prepare_librimix' in prepare_data.py. 3 csv files for train, val and test datasets are generated.

call 'dataio_prep', 'dataio_prep' reads the csv files defined in yaml config which are, in turn, generated by prepare_data.

The 'replacements' parameter of DynamicItemDataset.from_csv() is NOT really useful when there is NO 'data_root' in CSV files.

The dataset pipeline really cares 'id', 'mix_wav', 's1_wav', 's2_wav', 's3_wav' if mix3, 'noise_wav' if 'use_wham_noise'.

The dataset pipeline will output 'id', 'mix_sig', 's1_sig', 's2_sig', 's3_sig' if mix3, 'noise_sig' if 'use_wham_noise'.

'use_wham_noise: False' is defined in yaml config.

'dataio_prep' will return train_data, valid_data, test_data which are DynamicItemDataset.

check and load if pretrained model is specified in yaml config file.


## Main logic of yaml config

Some key config items:

seed: 1234 - also control the output folder

'use_wham_noise: False' controls if dataset has noise wav

'dynamic_mixing: False' controls if dynamic mixing
